<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="peter？"><meta name="keywords" content=""><meta name="description" content="方法所提出的方法主要在前三小节中描述：  条件VAE公式（a conditional VAE formulation） 基于变分推理的对准估计（alignment estimation derived from variational inference） 提高合成质量的对抗性训练（adversarial training for improving synthesis quality）  图1a"><meta property="og:type" content="article"><meta property="og:title" content="VITS 论文阅读-2"><meta property="og:url" content="http://petertan303.github.io/2023/06/14/VITS-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-2/index.html"><meta property="og:site_name" content="petertan303"><meta property="og:description" content="方法所提出的方法主要在前三小节中描述：  条件VAE公式（a conditional VAE formulation） 基于变分推理的对准估计（alignment estimation derived from variational inference） 提高合成质量的对抗性训练（adversarial training for improving synthesis quality）  图1a"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://petertan303.github.io/img/QQ%E6%88%AA%E5%9B%BE20230614191808.png"><meta property="og:image" content="http://petertan303.github.io/img/QQ%E6%88%AA%E5%9B%BE20230614185303.png"><meta property="article:published_time" content="2023-06-14T07:53:57.000Z"><meta property="article:modified_time" content="2023-06-25T14:56:53.744Z"><meta property="article:author" content="peter？"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://petertan303.github.io/img/QQ%E6%88%AA%E5%9B%BE20230614191808.png"><title>VITS 论文阅读-2 - petertan303</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"petertan303.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:"6a833fa5fd2900165acbe7570545a8a2",google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"mV6pM4FkddUR4CfQioaXxMrp-9Nh9j0Va",app_key:"YvR7UmmLPsWePIigEYzEjLuy",server_url:"https://mv6pm4fk.lc-cn-e1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>if(!Fluid.ctx.dnt){var _hmt=_hmt||[];!function(){var t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?6a833fa5fd2900165acbe7570545a8a2";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()}</script><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>PeterTan303</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="VITS 论文阅读-2"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-06-14 15:53" pubdate>2023年6月14日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>6.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>55 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">VITS 论文阅读-2</h1><div class="markdown-body"><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>所提出的方法主要在前三小节中描述：</p><ul><li>条件VAE公式（a conditional VAE formulation）</li><li>基于变分推理的对准估计（alignment estimation derived from variational inference）</li><li>提高合成质量的对抗性训练（adversarial training for improving synthesis quality）</li></ul><p>图1a和1b分别显示了我们方法的训练和推理过程。从现在起，我们将把我们的方法称为端到端文本到语音（VITS），具有对抗性学习的变分推理。</p><p>训练和推理流程如下：</p><p><img src="/img/QQ截图20230614191808.png" srcset="/img/loading.gif" lazyload alt=""></p><h2 id="条件推理（conditional-inference）"><a href="#条件推理（conditional-inference）" class="headerlink" title="条件推理（conditional inference）"></a>条件推理（conditional inference）</h2><p>条件VAE公式（a conditional VAE formulation）。</p><p>目标为一个”变分下界”，也叫证据下界（ELBO）。详细说就是“intractable marginal log-likelihood ”棘手边缘拟合对数似然的变分下界。</p><p>如图，$\log p<em>{\theta}(x \mid c)$的变分下界为$\mathbb{E}</em>{q<em>{\phi}(z \mid x)}\left[\log p</em>{\theta}(x \mid z)-\log \frac{q<em>{\phi}(z \mid x)}{p</em>{\theta}(z \mid c)}\right]$。</p><script type="math/tex;mode=display">L_{\text {recon }}=\left\|x_{mel}-\hat{x}_{mel}\right\|_{1}</script><p>下界：似然【数据点x的似然函数 - （近似后验分布 / 条件c下潜变量z的先验分布）的对数】</p><p>训练损失即负的ELBO。</p><p>也可以看作为重建损失 + KL散度，这在潜变量z服从近似后验分布时成立。</p><h3 id="重建损失"><a href="#重建损失" class="headerlink" title="重建损失"></a>重建损失</h3><p>作为重建损失中的目标数据点，我们使用<strong>mel频谱图</strong>而不是原始波形，由 $x<em>{mel}$ 表示。我们通过<strong>解码器</strong>将潜在变量z<strong>上采样到波形域</strong> $\hat{y}$ ，并将 $\hat{y}$ <strong>变换到融合谱图域</strong> $\hat{x}</em>{mel}$ 。然后，预测的和目标mel谱图之间的L1损失被用作重建损失：</p><script type="math/tex;mode=display">L_{\text {recon }}=\left\|x_{mel}-\hat{x}_{mel}\right\|_{1}</script><p>这可以被视为<strong>假设数据分布的拉普拉斯分布并忽略常数项</strong>的最大似然估计。我们定义了mel声谱图域中的重建损失，以通过使用近似人类听觉系统响应的mel标度来提高感知质量。注意，根据原始波形的mel谱图估计不需要可训练的参数，因为它只使用STFT和线性投影到mel标度上。<strong>此外，估计仅在训练期间使用，而不是推理。</strong> 在实践中，我们不对整个潜在变量z进行上采样，而是使用部分序列作为解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p><h3 id="KL收敛"><a href="#KL收敛" class="headerlink" title="KL收敛"></a>KL收敛</h3><p>先验编码器c的输入条件由从文本中提取的音素$c_{text}$和音素与潜在变量之间的对齐 A 组成。</p><p>对齐是一个硬单调注意力矩阵，其$\mid c_{text}\mid \times \mid z\mid$维度表示每个输入音素扩展到与目标语音时间对齐的长度。由于对齐没有基本事实标签，我们必须在每次训练迭代时估计对齐。</p><p>在我们的问题设置中，我们的目标是为后验编码器提供更多的高分辨率信息。因此，我们使用目标语音$x_{lin}$的线性尺度频谱图作为输入，而不是mel频谱图。注意，修改后的输入并不违反变分推理的性质。那么KL分歧是：</p><p><img src="/img/QQ截图20230614185303.png" srcset="/img/loading.gif" lazyload alt=""></p><p>“因子分解正态分布”用于参数化我们的先验和后验编码器。</p><p>我们发现，增加先验分布的表现力对于生成真实样本很重要。因此，我们应用归一化流 f ，该流允许在因子分解的正态先验分布之上，根据变量变化规则，将简单分布可逆变换为更复杂的分布。</p><h2 id="路线估计（Alignment-Estimation）"><a href="#路线估计（Alignment-Estimation）" class="headerlink" title="路线估计（Alignment Estimation）"></a>路线估计（Alignment Estimation）</h2><p>基于变分推理的对准估计（alignment estimation derived from variational inference）。</p><h3 id="单调对齐搜索（MONOTONIC-ALIGNMENT-SEARCH）"><a href="#单调对齐搜索（MONOTONIC-ALIGNMENT-SEARCH）" class="headerlink" title="单调对齐搜索（MONOTONIC ALIGNMENT SEARCH）"></a>单调对齐搜索（MONOTONIC ALIGNMENT SEARCH）</h3><p>为了估计输入文本和目标语音之间的对齐A，我们采用<strong>单调对齐搜索（MAS）</strong>。</p><p>这是一种搜索对齐的方法，其最大化了由归一化流 f 参数化的数据的可能性。</p><p>因为人类按顺序阅读文本，不跳过任何单词，候选比对（ candidate alignments）被限制为单调且不跳过。</p><p>为了找到最佳对准，Kim等人（2020）使用动态规划。在我们的这个情况下直接应用MAS是困难的，因为我们的目标是ELBO，而不是确切的对数似然。因此，我们重新定义MAS，以找到最大化ELBO的对齐。</p><p>这个过程简化为找到最大化潜在变量z的对数似然的对齐。</p><p>但事实上，无论修不修改，都可以工作。因此我们使用的是原始MAS。</p><h3 id="文本时长预测器（DURATION-PREDICTION-FROM-TEXT）"><a href="#文本时长预测器（DURATION-PREDICTION-FROM-TEXT）" class="headerlink" title="文本时长预测器（DURATION PREDICTION FROM TEXT）"></a>文本时长预测器（DURATION PREDICTION FROM TEXT）</h3><p>我们可以通过对估计的对齐$\sum<em>{j}^{} A</em>{i,j}$的每行中的所有列求和来计算每个输入token$d_i$的持续时间。但持续时间可以用来训练确定性的持续时间预测器，但它不能表达一个人每次以不同的语速说话的方式。</p><p>为了生成类似人类的语音节奏，我们设计了一个<strong>随机持续时间预测器</strong>，使其样本遵循给定音素的持续时间分布。</p><p>随机持续时间预测器是一种基于流的生成模型，通常通过最大似然估计进行训练。然而，最大似然估计的直接应用是困难的，因为每个输入音素的持续时间是</p><p>1）离散整数，其需要被去量化（dequantized）以使用连续归一化流。</p><p>2）标量，其由于可逆性而无法进行高维变换。</p><p>我们应用<strong>变分去量化</strong>和<strong>变分数据扩充</strong>来解决这些问题。</p><p>具体地说，我们引入了两个随机变量 u 和 v ，它们具有与持续时间序列 d 相同的时间分辨率和维度，分别用于变分去方程化和变分数据扩充。</p><p>我们将u的支持度限制为[0, 1），使得差$d-v$变成了一个正实数序列。</p><p>我们按通道连接 v 和 d ，以生成更高维的潜在表示。、</p><p>我们通过近似后验分布$q(u,v|d,c<em>{text})$对这两个变量进行采样。由此产生的目标是<strong>音素持续时间的对数似然的变分下界</strong>。训练损失$L</em>{dur}$是<strong>负变分下界</strong>。</p><p>我们将阻止输入梯度反向传播的”停止梯度算子”应用于输入s，保证持续时间预测器的训练不会影响其他模块的训练。</p><p>而取样程序相对简单：通过随机持续时间预测器的逆变换，从随机噪声中采样音素持续时间，然后将其转换为整数。</p><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><p>提高合成质量的对抗性训练（ adversarial training for improving synthesis quality）。</p><p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器D，用于区分解码器G生成的输出和实际的波形y。</p><p>在这项工作中，我们使用了两种成功应用于语音合成的损失类型：一种是<strong>对抗性训练的最小二乘损失函数</strong>，另一种是<strong>训练生成器的附加特征匹配损失</strong>。</p><p>T表示鉴别器中的层的总数，并且$D^{l}$输出具有$N_{l}$个特征的鉴别器的第l层的特征图。</p><p>值得注意的是，特征匹配损失可以被视为在<strong>鉴别器的隐藏层中测量的重建损失</strong>，该重建损失被建议作为VAE的逐元素重建损失的替代方案。</p><h2 id="最终的损失函数"><a href="#最终的损失函数" class="headerlink" title="最终的损失函数"></a>最终的损失函数</h2><p>所有损失函数直接相加。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>总体架构由后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器组成。后验编码器和鉴别器仅用于训练，而不用于推理。</p><h3 id="后验编码器"><a href="#后验编码器" class="headerlink" title="后验编码器"></a>后验编码器</h3><p>对于后验编码器，我们使用WaveGlow和Glow-TTS中使用的非因果WaveNet残差块, “the non-causal WaveNet residual blocks”。</p><p>WaveNet残差块由具有多个扩张卷积层（ dilated convolutions ），每层含有门控激活单元（gated activation unit）和跳跃连接（ skip connection）。块上方的线性投影层产生正态后验分布的均值和方差。</p><p>对于多个说话人的情况，我们在残差块中使用全局条件反射（global conditioning）来添加说话人embedding。</p><h3 id="先验编码器"><a href="#先验编码器" class="headerlink" title="先验编码器"></a>先验编码器</h3><p>先验编码器包括处理输入音素$c_{text}$的<strong>文本编码器</strong>、改进先验分布的灵活性的<strong>归一化流</strong> f 。</p><p>文本编码器是一种transformer编码器（transformer encoder），它使用相对位置表示（relative positional representation）而不是绝对位置编码（absolute positional encoding）。</p><p>我们可以通过文本编码器和文本编码器上方的线性投影层，从 $c<em>{text}$ 中获得 hidden representation $h</em>{text}$ ，该线性投影层产生用于构建先验分布的均值和方差。</p><p>归一化流(normalizing flow)是仿射耦合层（affine coupling layers）堆积而成，包含 WaveNet 残差块的堆栈。为了简单起见，我们将归一化流设计为雅可比行列式为1的保体积变换（a volume-preserving transformation with the Jacobian determinant of one）。</p><p>这个变换来自于 GLOW. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03039">https://arxiv.org/abs/1807.03039</a></p><p>对于多说话人设置，我们通过全局条件，将说话人embedding加入到归一化流中的残差块中。</p><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器本质上是<strong>HiFi GAN V1生成器</strong>。它由”反条件姿态卷积”堆积组成，每个卷积后面，都有一个多接收场融合模块（MRF）。</p><p>MRF的输出是具有不同感受野大小的残差块的输出之和。</p><p>对于多说话人设置，我们添加一个转换说话人embedding的线性层，并将其添加到输入潜在变量z中。</p><h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><p>我们遵循HiFi GAN中提出的<strong>多周期鉴别器的鉴别器架构</strong>。</p><p>多周期判别器是基于<strong>马尔可夫窗的子鉴别器的混合</strong>，每个子判别器对输入波形的不同周期模式进行操作。</p><h3 id="随机持续时间预测器"><a href="#随机持续时间预测器" class="headerlink" title="随机持续时间预测器"></a>随机持续时间预测器</h3><p>随机持续时间预测器根据条件输入的 $h_{text}$ 估计音素持续时间的分布。</p><p>为了有效地参数化随机持续时间预测器，我们将<strong>残差块</strong>，与<strong>扩张和深度可分离的卷积层</strong>叠加。我们还将神经样条流（neural spline flows）应用于耦合层，其通过使用单调有理二次样条（monotonic rational-quadratic splines）采用可逆非线性变换的形式。</p><p>与常用的仿射耦合层相比，神经样条流的参数相似数量，但提高了变换的表现力。</p><p>对于多说话人设置，我们添加了一个线性层来转换说话人embedding，并将其添加到输入 $h_{text}$ 中。</p><h1 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h1><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>使用了：</p><ul><li>后验编码器</li><li>先验编码器</li><li>解码器</li><li>鉴别器</li><li>随机持续时间预测器</li></ul><p>也就是所有部件。</p><p>$x_{lin}$ 为线性频谱, 通过后验编码器, flow, 解码器, 输出预测的原始波形 $\hat{y}$ .</p><p>$c<em>{text}$ 为从文本中提取的音素, 通过文本编码器得到 $h</em>{text}$ , 投影到 z 所服从的正态分布, 正态分布的两个参数是 $\mu<em>{\theta}$ 和 $\sigma</em>{\theta}$ . 然后使用单调对齐搜索( MAS )估计潜在变量之间的对齐 A ,最大化由归一化流 f 参数化的数据的可能性.</p><p>对齐 A 是一个<strong>硬单调注意力矩阵</strong>，其$\mid c<em>{text}\mid \times \mid z\mid$维度, 表示每个输入音素$c</em>{text}$扩展到与目标语音时间对齐的长度。</p><p>核心的 flow 是一堆 z 和一堆对应的 $f<em>z(z)$ .对于 $\theta$ 层的 $z$, 服从的正态分布系数是 $\mu</em>{\theta}$ 和 $\sigma_{\theta}$ ,</p><p>$f_{\theta}(z)$ ,</p><h2 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h2><p>使用了：</p><ul><li>先验编码器</li><li>解码器</li><li>随机持续时间预测器</li></ul><p>也就是不使用后验编码器和判别器。</p><p>$c_{text}$ ，phoneme（音素），作为输入。</p><p>经过text encoder，文本编码器，和线性投影层，得到 $h_{text}$ ，hidden representation，和投影。</p><p>$h_{text}$ 使用随机时间预测器，逆变换，从随机噪声中提取持续时间。</p><p>取样程序：通过随机持续时间预测器的逆变换，从随机噪声中采样音素持续时间，然后将其转换为整数，得到向量 $d$ 。</p><p>然后投入先验编码器，先验编码器包括文本编码器和归一化流，文本编码器是一个transformer，归一化流进行估计，参数化对先验概率q的逼近。</p><h1 id="涉及的论文与概念"><a href="#涉及的论文与概念" class="headerlink" title="涉及的论文与概念"></a>涉及的论文与概念</h1><h2 id="归一化流"><a href="#归一化流" class="headerlink" title="归一化流"></a>归一化流</h2><p>本质是一个生成模型。一个基于可能性的生成模型。</p><p>（自回归模型, 生成对抗网络(GAN)的一部分, 变分自动编码器（VAE）也是自回归模型。）</p><p>Flow 将简单分布（易于采样和评估密度）映射到复杂分布（通过数据学习）。但是方便起见，实际使用时就是想办法得到一个encoder将输入x编码为<strong>隐变量z</strong>，并且<strong>使得z服从标准正态分布</strong>。</p><p>得益于flow模型的精巧设计，这个encoder是<strong>可逆</strong>的，从而我们可以立马从encoder写出相应的decoder（生成器）出来，因此，只要encoder训练完成，我们就能同时得到decoder，完成生成模型的构建。</p><hr><p>x 和 z 应该有以下关系：</p><p>首先， $X=f(Z)$ ， $Z=f^{-1}(X)$ 。也就是说从 Z 映射到 X ，这个映射确定而可逆。</p><p>然后有：</p><p><strong>变量变换定理(change of variable theorem):</strong> 有 $p_z(z)$ 和 $p_x(x)$ ,有 $f(z)=x$ ,如果要通过 $f$ 建立 $p_z(z)$ 和 $p_x(x)$ 之间的关系，可以将 $p_z(z)$ 和 $p_x(x)$ 使用微分方法借取一小段，直接计算一小段的体积，并对比，并按照体积的比例进行缩放，缩放函数在每个点上组合起来就是 $f(z)=x$ 。</p><script type="math/tex;mode=display">p_x(x)=p_z(z) ( f^{-1}(x)) \mid {det}( \frac{\partial f^{-1}(x)}{\partial x} =p_z(z) ( f^{-1}(x)) \mid {det}( \frac{\partial f(x)}{\partial x}^{-1}  ) \mid</script><p>其中，由于可逆矩阵 $det(A^{-1})=det(A)^{-1}$ 。因此对于 $z=f^{-1}(x)$ ，有：</p><script type="math/tex;mode=display">p_x(x)=p_z(z) \mid {det}( \frac{\partial f(z)}{\partial z}  ) \mid</script><p>对于 $\frac{\partial f^{-1}(x)}{\partial x}$ ，这是一个 n 维矩阵。这个矩阵记做<strong>雅可比矩阵</strong>。</p><p><strong>保体积</strong>： $\mid {det}( \frac{\partial f(z)}{\partial z} ) \mid =1$ ，则从 z 到 x 的映射是保体积的。换言之，变换后的 $p_x$ 和 原始的 $p_z$ 有同样的体积，或者说volume。</p><p>(体积指的就是行列式.)</p><p>流模型核心条件：</p><ul><li>模型可逆</li><li>对应的雅可比行列式容易计算<ul><li>因此本文将其设计为雅可比行列式为1的保体积变换（a volume-preserving transformation with the Jacobian determinant of one）</li></ul></li></ul><p>有“边界可能性” $p_x(x)$ ，满足</p><script type="math/tex;mode=display">p_x(x;\theta )=p_z(f_{\theta }^{-1}(x)) \mid {det}( \frac{\partial f_{\theta }^{-1}(x)}{\partial x}  ) \mid</script><p>名称“归一化流”可以解释如下：</p><ul><li>“归一化”意味着变量的变化在应用可逆变换后给出归一化密度（normalized density）。</li><li>“流”意味着可逆变换可以相互组合，以创建更复杂的可逆变换。</li></ul><p>存在多个隐变量 z, 存在对应数量的变换函数 f(z), 而最后一个 z 对应出来就是 x.</p><hr><p>优化函数:</p><script type="math/tex;mode=display">logp_k(x^i)=logp_z(z^i)+\sum_{h=1}^{K}log\mid det(J_{G^{-1}})\mid =logp_z(G^{-1}(x^i))+\sum_{h=1}^{K}log\mid det(J_{G^{-1}})\mid</script><p>这是 $G^{-1}$ 训练的目标: 最大化这个式子的值即可.</p><p>实际使用 G, 将其逆转即可.</p><h3 id="平面流"><a href="#平面流" class="headerlink" title="平面流"></a>平面流</h3><h3 id="vits使用的流模型"><a href="#vits使用的流模型" class="headerlink" title="vits使用的流模型"></a>vits使用的流模型</h3><p><strong>the non-causal WaveNet residual blocks</strong> used in <strong>WaveGlow</strong> (Prenger et al., 2019) and <strong>Glow-TTS</strong> (Kim et al., 2020).</p></div><hr><div><div class="post-metas my-3"></div><div class="license-box my-3"><div class="license-title"><div>VITS 论文阅读-2</div><div>http://petertan303.github.io/2023/06/14/VITS-论文阅读-2/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>peter？</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年6月14日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2023/06/14/VITS-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-3/" title="VITS 论文阅读-3"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">VITS 论文阅读-3</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2023/06/13/fpga-%E7%94%B5%E6%A2%AF%E9%A1%B9%E7%9B%AE/" title="fpga 电梯项目"><span class="hidden-mobile">fpga 电梯项目</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>